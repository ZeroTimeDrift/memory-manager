#!/usr/bin/env npx ts-node

/**
 * Semantic Deduplication for Memory Captures
 * 
 * Prevents the same information from being written multiple times across
 * daily logs, MEMORY.md, and topic files. Uses character trigram similarity
 * (Jaccard index) which handles paraphrases better than keyword matching.
 * 
 * Three similarity tiers:
 *   - EXACT:    normalized strings match           â†’ always skip
 *   - HIGH:     trigram similarity >= 0.65          â†’ skip (likely same info)
 *   - MODERATE: trigram similarity >= 0.45          â†’ warn (possible dupe)
 * 
 * Usage (programmatic):
 *   import { checkDuplicate, DedupResult } from './dedup';
 *   const result = checkDuplicate("Built health dashboard with scoring");
 *   if (result.isDuplicate) console.log(`Dupe of: ${result.bestMatch.source}`);
 * 
 * Usage (CLI):
 *   echo "Built health dashboard" | npx ts-node src/dedup.ts
 *   npx ts-node src/dedup.ts "Some text to check"
 *   npx ts-node src/dedup.ts --scan          # Scan all recent captures for dupes
 *   npx ts-node src/dedup.ts --json "text"   # Machine-readable output
 */

import * as fs from 'fs';
import * as path from 'path';

const WORKSPACE = '/root/clawd';
const MEMORY_FILE = path.join(WORKSPACE, 'MEMORY.md');
const DAILY_DIR = path.join(WORKSPACE, 'memory', 'daily');
const TOPICS_DIR = path.join(WORKSPACE, 'memory', 'topics');

// â”€â”€â”€ Types â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export interface DedupMatch {
  text: string;          // The matching line
  source: string;        // File where match was found
  similarity: number;    // 0.0â€“1.0 trigram Jaccard similarity
  tier: 'exact' | 'high' | 'moderate';
}

export interface DedupResult {
  isDuplicate: boolean;      // true if HIGH or EXACT match found
  isWarning: boolean;        // true if MODERATE match found (not blocked)
  bestMatch: DedupMatch | null;
  allMatches: DedupMatch[];  // All matches above moderate threshold
  checkedLines: number;      // How many lines were compared
  checkedFiles: number;      // How many files were scanned
  elapsedMs: number;
}

export interface DedupScanResult {
  duplicates: Array<{
    line: string;
    source: string;
    duplicateOf: string;
    duplicateSource: string;
    similarity: number;
  }>;
  totalLines: number;
  totalDuplicates: number;
}

// â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const THRESHOLDS = {
  exact: 0.92,     // Normalized near-exact match
  high: 0.60,      // Strong semantic overlap â€” skip capture
  moderate: 0.42,  // Possible dupe â€” warn but don't block
};

// How many recent daily files to check (performance bound)
const MAX_DAILY_FILES = 7;

// Minimum line length to consider for comparison (skip headers, dates, etc.)
const MIN_LINE_LENGTH = 20;

// â”€â”€â”€ Similarity Engine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Normalize text for comparison: lowercase, strip markdown, collapse whitespace
 */
function normalize(text: string): string {
  return text
    .toLowerCase()
    .replace(/^[-*>#\d.]+\s*/gm, '')    // Strip list markers, headers, blockquotes
    .replace(/\*\*|__|\*|_|`|~~/g, '')  // Strip inline markdown
    .replace(/\[([^\]]*)\]\([^)]*\)/g, '$1') // Links â†’ text
    .replace(/\b\d{2}:\d{2}\b/g, '')     // Strip timestamps
    .replace(/\b\d{4}-\d{2}-\d{2}\b/g, '') // Strip dates
    .replace(/\([^)]*\)/g, '')           // Strip parentheticals (paths, scores, etc.)
    .replace(/\s+/g, ' ')
    .trim();
}

/**
 * Generate character trigrams from text
 */
function trigrams(text: string): Set<string> {
  const result = new Set<string>();
  const normalized = normalize(text);
  if (normalized.length < 3) return result;
  
  for (let i = 0; i <= normalized.length - 3; i++) {
    result.add(normalized.substring(i, i + 3));
  }
  return result;
}

/**
 * Extract significant words (4+ chars) from text
 */
function significantWords(text: string): Set<string> {
  return new Set(
    normalize(text)
      .split(/\s+/)
      .filter(w => w.length >= 4 && !/^\d+$/.test(w))
  );
}

/**
 * Multi-signal similarity: combines trigram similarity with word containment.
 * 
 * Handles the key weakness of pure Jaccard: when texts differ in length,
 * Jaccard penalizes because the union is large. We use:
 * - Trigram Jaccard (captures character-level structure)
 * - Trigram containment (shorter text's trigrams found in longer)
 * - Word containment (significant words from shorter found in longer)
 * 
 * Final score: weighted blend that handles paraphrases and length asymmetry.
 */
function combinedSimilarity(a: Set<string>, aWords: Set<string>, b: Set<string>, bWords: Set<string>): number {
  if (a.size === 0 && b.size === 0) return 1.0;
  if (a.size === 0 || b.size === 0) return 0.0;
  
  // Trigram intersection
  let trigramInt = 0;
  for (const gram of a) {
    if (b.has(gram)) trigramInt++;
  }
  
  // Trigram Jaccard
  const trigramJaccard = trigramInt / (a.size + b.size - trigramInt);
  
  // Trigram containment (max direction)
  const trigramContAB = a.size > 0 ? trigramInt / a.size : 0;
  const trigramContBA = b.size > 0 ? trigramInt / b.size : 0;
  const trigramCont = Math.max(trigramContAB, trigramContBA);
  
  // Word containment (max direction)
  let wordInt = 0;
  for (const w of aWords) {
    if (bWords.has(w)) wordInt++;
  }
  const wordContAB = aWords.size > 0 ? wordInt / aWords.size : 0;
  const wordContBA = bWords.size > 0 ? wordInt / bWords.size : 0;
  const wordCont = Math.max(wordContAB, wordContBA);
  
  // Weighted blend: 25% Jaccard + 35% trigram containment + 40% word containment
  // Word containment is weighted highest because it captures semantic paraphrases best
  return 0.25 * trigramJaccard + 0.35 * trigramCont + 0.40 * wordCont;
}

/**
 * Quick normalized string equality (handles formatting differences)
 */
function normalizedEqual(a: string, b: string): boolean {
  return normalize(a) === normalize(b);
}

// â”€â”€â”€ Corpus Loading â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

interface CorpusLine {
  text: string;
  source: string;
}

/**
 * Extract meaningful lines from a markdown file.
 * Skips frontmatter, headers, empty lines, and very short lines.
 */
function extractLines(content: string, source: string): CorpusLine[] {
  const lines: CorpusLine[] = [];
  let inFrontmatter = false;
  
  for (const raw of content.split('\n')) {
    const trimmed = raw.trim();
    
    // Skip YAML frontmatter
    if (trimmed === '---') {
      inFrontmatter = !inFrontmatter;
      continue;
    }
    if (inFrontmatter) continue;
    
    // Skip empty, headers, and short lines
    if (!trimmed) continue;
    if (trimmed.startsWith('#')) continue;
    
    // Strip list markers for length check
    const clean = trimmed.replace(/^[-*>]+\s*/, '').replace(/^\d+\.\s*/, '');
    if (clean.length < MIN_LINE_LENGTH) continue;
    
    lines.push({ text: clean, source });
  }
  
  return lines;
}

/**
 * Load the comparison corpus: recent daily files + MEMORY.md + topic files
 */
function loadCorpus(options?: { skipFile?: string }): CorpusLine[] {
  const corpus: CorpusLine[] = [];
  
  // 1. MEMORY.md
  if (fs.existsSync(MEMORY_FILE)) {
    const content = fs.readFileSync(MEMORY_FILE, 'utf-8');
    corpus.push(...extractLines(content, 'MEMORY.md'));
  }
  
  // 2. Recent daily files (last N days)
  if (fs.existsSync(DAILY_DIR)) {
    const files = fs.readdirSync(DAILY_DIR)
      .filter(f => f.endsWith('.md'))
      .sort()
      .reverse()
      .slice(0, MAX_DAILY_FILES);
    
    for (const file of files) {
      const filePath = path.join(DAILY_DIR, file);
      if (options?.skipFile && filePath === options.skipFile) continue;
      const content = fs.readFileSync(filePath, 'utf-8');
      corpus.push(...extractLines(content, `daily/${file}`));
    }
  }
  
  // 3. Topic files
  if (fs.existsSync(TOPICS_DIR)) {
    const files = fs.readdirSync(TOPICS_DIR)
      .filter(f => f.endsWith('.md'));
    
    for (const file of files) {
      const filePath = path.join(TOPICS_DIR, file);
      const content = fs.readFileSync(filePath, 'utf-8');
      corpus.push(...extractLines(content, `topics/${file}`));
    }
  }
  
  return corpus;
}

// â”€â”€â”€ Public API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Check if a piece of text is a duplicate of existing memory content.
 * 
 * @param text - The text to check
 * @param options - Optional: skipFile (don't check this file path)
 * @returns DedupResult with match details
 */
export function checkDuplicate(text: string, options?: { skipFile?: string }): DedupResult {
  const startTime = Date.now();
  
  const normalizedInput = normalize(text);
  if (normalizedInput.length < MIN_LINE_LENGTH) {
    return {
      isDuplicate: false,
      isWarning: false,
      bestMatch: null,
      allMatches: [],
      checkedLines: 0,
      checkedFiles: 0,
      elapsedMs: Date.now() - startTime,
    };
  }
  
  const inputTrigrams = trigrams(text);
  const inputWords = significantWords(text);
  const corpus = loadCorpus(options);
  const allMatches: DedupMatch[] = [];
  const seenFiles = new Set<string>();
  
  for (const line of corpus) {
    seenFiles.add(line.source);
    
    // Quick exact check first
    if (normalizedEqual(text, line.text)) {
      allMatches.push({
        text: line.text,
        source: line.source,
        similarity: 1.0,
        tier: 'exact',
      });
      continue;
    }
    
    // Multi-signal similarity
    const lineTrigrams = trigrams(line.text);
    const lineWords = significantWords(line.text);
    const sim = combinedSimilarity(inputTrigrams, inputWords, lineTrigrams, lineWords);
    
    if (sim >= THRESHOLDS.moderate) {
      let tier: DedupMatch['tier'];
      if (sim >= THRESHOLDS.exact) tier = 'exact';
      else if (sim >= THRESHOLDS.high) tier = 'high';
      else tier = 'moderate';
      
      allMatches.push({
        text: line.text,
        source: line.source,
        similarity: sim,
        tier,
      });
    }
  }
  
  // Sort by similarity descending
  allMatches.sort((a, b) => b.similarity - a.similarity);
  
  const bestMatch = allMatches.length > 0 ? allMatches[0] : null;
  const isDuplicate = bestMatch !== null && (bestMatch.tier === 'exact' || bestMatch.tier === 'high');
  const isWarning = !isDuplicate && bestMatch !== null && bestMatch.tier === 'moderate';
  
  return {
    isDuplicate,
    isWarning,
    bestMatch,
    allMatches,
    checkedLines: corpus.length,
    checkedFiles: seenFiles.size,
    elapsedMs: Date.now() - startTime,
  };
}

/**
 * Batch check: find all duplicates within recent daily files.
 * Useful for auditing existing content.
 */
export function scanForDuplicates(): DedupScanResult {
  const corpus = loadCorpus();
  const duplicates: DedupScanResult['duplicates'] = [];
  
  // Compare each line against all lines that came BEFORE it (chronological dedup)
  for (let i = 1; i < corpus.length; i++) {
    const current = corpus[i];
    const currentTrigrams = trigrams(current.text);
    const currentWords = significantWords(current.text);
    
    for (let j = 0; j < i; j++) {
      const earlier = corpus[j];
      
      // Skip self-file comparisons (within same file is fine)
      if (current.source === earlier.source) continue;
      
      const sim = combinedSimilarity(currentTrigrams, currentWords, trigrams(earlier.text), significantWords(earlier.text));
      
      if (sim >= THRESHOLDS.high) {
        duplicates.push({
          line: current.text.substring(0, 80),
          source: current.source,
          duplicateOf: earlier.text.substring(0, 80),
          duplicateSource: earlier.source,
          similarity: sim,
        });
        break; // One match is enough per line
      }
    }
  }
  
  return {
    duplicates,
    totalLines: corpus.length,
    totalDuplicates: duplicates.length,
  };
}

// â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function main(): Promise<void> {
  const args = process.argv.slice(2).filter(a => !a.startsWith('--'));
  const jsonMode = process.argv.includes('--json');
  const scanMode = process.argv.includes('--scan');
  
  if (scanMode) {
    console.log('ğŸ” Scanning for duplicates across memory files...\n');
    const result = scanForDuplicates();
    
    if (jsonMode) {
      console.log(JSON.stringify(result, null, 2));
      return;
    }
    
    if (result.duplicates.length === 0) {
      console.log(`âœ… No duplicates found across ${result.totalLines} lines.`);
      return;
    }
    
    console.log(`âš ï¸  Found ${result.totalDuplicates} duplicates across ${result.totalLines} lines:\n`);
    
    for (const dupe of result.duplicates) {
      console.log(`  ğŸ“„ ${dupe.source}: "${dupe.line}"`);
      console.log(`  ğŸ”„ â‰ˆ${(dupe.similarity * 100).toFixed(0)}% of ${dupe.duplicateSource}: "${dupe.duplicateOf}"`);
      console.log('');
    }
    
    return;
  }
  
  // Single text check
  let input = '';
  
  if (args.length > 0) {
    input = args.join(' ');
  } else {
    input = await new Promise<string>((resolve) => {
      let data = '';
      if (process.stdin.isTTY) {
        console.log('âŒ¨ï¸  Enter text to check for duplicates (Ctrl+D to finish):');
      }
      process.stdin.setEncoding('utf-8');
      process.stdin.on('data', (chunk) => { data += chunk; });
      process.stdin.on('end', () => { resolve(data); });
      setTimeout(() => { if (!data) resolve(''); }, 5000);
    });
  }
  
  input = input.trim();
  if (!input) {
    console.log('âš ï¸  No input provided.');
    console.log('Usage: echo "text" | npx ts-node src/dedup.ts');
    console.log('       npx ts-node src/dedup.ts --scan');
    process.exit(1);
  }
  
  const result = checkDuplicate(input);
  
  if (jsonMode) {
    console.log(JSON.stringify(result, null, 2));
    return;
  }
  
  console.log('');
  if (result.isDuplicate) {
    console.log(`ğŸ”„ DUPLICATE DETECTED (${result.bestMatch!.tier})`);
    console.log(`   Similarity: ${(result.bestMatch!.similarity * 100).toFixed(0)}%`);
    console.log(`   Matches: "${result.bestMatch!.text.substring(0, 80)}"`);
    console.log(`   Source: ${result.bestMatch!.source}`);
  } else if (result.isWarning) {
    console.log(`âš ï¸  POSSIBLE DUPLICATE (moderate match)`);
    console.log(`   Similarity: ${(result.bestMatch!.similarity * 100).toFixed(0)}%`);
    console.log(`   Matches: "${result.bestMatch!.text.substring(0, 80)}"`);
    console.log(`   Source: ${result.bestMatch!.source}`);
  } else {
    console.log(`âœ… No duplicates found.`);
  }
  
  console.log(`   Checked: ${result.checkedLines} lines across ${result.checkedFiles} files (${result.elapsedMs}ms)`);
  
  if (result.allMatches.length > 1) {
    console.log(`\n   Other matches:`);
    for (const m of result.allMatches.slice(1, 4)) {
      console.log(`     ${(m.similarity * 100).toFixed(0)}% [${m.tier}] ${m.source}: "${m.text.substring(0, 60)}"`);
    }
  }
}

if (require.main === module) {
  main().catch(e => {
    console.error('âŒ Error:', e.message);
    process.exit(1);
  });
}
